# ABHIJIT GOKHALE
*abhijit.s.gokhale@gmail.com | +1-(315) 278-4351 | [LinkedIn](https://www.linkedin.com/in/abhijitgokhale/) | [GitHub](https://github.com/AbhijitGokhale) | [HackerRank](https://www.hackerrank.com/agokhale4) | [PLUM Behavioral Profile](https://secure.plum.io/en/p/H9mMpam2DX7DlEJ9LST0mw)*

Senior Data Engineer, JP Morgan Chase & Co. | Data Scientist Intern, Caterpillar | Data Analyst, Accenture | Data Science Teaching Assistant, Syracuse University

# PROFILE SUMMARY
As a senior data engineer at JPMorgan Chase & Co., I applied AWS Glue ETL skills to design, develop, and test data warehousing solutions to enhance customer identity and authentication. I received recognition from business stakeholders and product owners for maintening end to end data pipeline with data resilliency, zero defect and prompt delivery. In this journey, I worked on data mapping, data validation, exploratory data analysis, efficient and fast data migration, data deployment.

I completed my Master's degree in Applied Data Science from Syracuse University School of Information Studies, where I also worked as a teaching assistant for IST 687: Introduction to Data Science. I helped students with assignments, grading, and resolving queries on statistical results, implementation, and R programming. Additionally, I interned as a data scientist at Caterpillar, where I used Python and R to explore, analyze, and visualize datasets related to machine regular events and machine sensors. Working as a data analyst with Accenture on multiple client focussed, development and testing opportunities with pharmaceutical and energy production client's financial as well as HR data, layed a base to my data & analytics journey. I am passionate about working with huge data sets and finding insights that can improve business strategy and outcomes. I am a team player with excellent communication and problem-solving skills.


# EDUCATION
* Masters of Science (Data Science), *Syracuse University*, 2021-2022
* Bachelors of Science (Data Science), *Mumbai University*, 2012-2016


# LEADERSHIP
***Senior Data Engineer, JP Morgan Chase & Co. (Feb 2023 - Current)***
* Recommended time and cost saving automated solutions and frameworks in handling data transformation with PySpark and AWS to reduce total efforts by 20%.
* Conducted learning sessions with team members to explain the implementation of instance graph data model using openCypher, Glue Crawlers, Alteryx and Tableu dashboard development.
  
***Data Science Teaching Assistant, Syracuse University (Sept 2021 – Dec 2022)***
* Delivered assistance to program director and professors, in interpreting statistical and machine learning results in R programming with 9 classes of 30 students each for 22 labs, and 3 projects for Introduction to Data Science course. 
* Analyzed 8k health insurance data to recommend vital features for expensive prices such as smoking, location, age, exercise. 
* Implemented quantitative research on data pipeline project implementation and development using Alteryx to run python scripts and guide students on different stages of data preprocessing and analysis. 
* Designed ETL systems in AWS to extract data from S3 using Glue Crawlers and delivered data load as well as CRUD operations on Glue Data tables using SQL queries in Athena & Redshift. 



# WORK EXPERIENCE
***Senior Data Engineer, JP Morgan & Chase Co. (Feb 2023 - Current)***                                                                                           	
* Engineered end-to-end big data pipelines using PySpark and AWS Glue to process ~150GB daily across 10+ sources, and automated deployment of 40+ ETL modules via Terraform, managing resources like IAM, S3, Athena, and SNS for scalable and auditable production workflows.
* Architected a graph-based data model in AWS Neptune with 100M+ nodes to visualize customer relationships and attributes, driving a 20% increase in stakeholder engagement through enhanced data exploration.
* Built a reusable data processing and quality framework with parallel processing and centralized config management via AuroraDB and S3, reducing maintenance overhead by 25%.
* Optimized ingestion workflows by automating Glue Crawler and Athena catalog updates for curated datasets, reducing data latency by 20% and enabling timely updates for analytical reporting.
* Built a reusable PySpark log monitoring framework to capture job status, exceptions, and failure metadata; integrated with Athena to support operational Tableau reporting and accelerate failure diagnostics by 40%.
* Developed stakeholder-facing Tableau dashboards to visualize ETL data pipeline performance metrics and ML model generated risk scoring trends, enhancing data transparency and improving decision-making efficiency by 35%.

***Data Scientist Intern, Caterpillar, Chicago, Illinois (May 2022 – Aug 2022)***	
* Analyzed 341 machine’s engine removal data using python pandas to predict survival or risk probability of engine shutdown.
* Performed feature selection using nonparametric log rank test with Kaplan-Meier, correlation analysis, & feature importance.
* Delivered machine learning prototype, with Git version control, consist of stratification data sampling, hyperparameter tuning using grid-search 5-fold cross validation for Survival Analysis supervised learning algorithms such as Extra Survival Trees, Random Survival Forest, Gradient Boosting, Cox-PH to get best evaluation Concordance Index ( > 0.5 ) and Brier Score. 
* Recommended probability of survival / risk with significant features (P-value < 0.05) by simulating 12% of test data samples.

***Data Analyst, Accenture, Mumbai, India (Dec 2016 – Aug 2020)***	
* Developed & enhanced Informatica ETL jobs on 4+ mil. data to perform big data analysis – data partitioning, clustered index, reusable database objects with DDL, DML, DCL, & TCL SQL operations using Agile methodology & CI/CD approach.
* Designed SDLC including logical data modeling for 20+ databases & 100 + tables to maintain data quality, and data privacy.
* Led a 6 - analysts team to deploy ETL jobs from QA to PROD linked with 100+ Jira items with SAT & UAT executions.
* Improved the SSIS project flow by 30% and resolved 60% of major daily batch job issues using MS SQL server and HiveQL.
* Automated exploratory data analysis using Python for 15 raw files & delivered key performance indicators to maximize profit, revenue by 20 dashboards & reports using MS Power BI & Excel  - solver optimization & DAX functionality.


# PROJECTS
Below projects highlight business and technical skills in Data Engineering, Data Analytics, and Machine Learning using Python, PySpark, R, SQL, NoSQL, Tableau, and MS Power BI.


## [Credit Card Fraud Detection](https://github.com/AbhijitGokhale/Credit-Card-Fraud-Detetction)
**Objective:**  Utilize existing credit card fraud data from January 2019 to December 2020 and develop a machine learning model in PySpark to perform Big Data Analysis which inturn leads to the best prediction results in revealing and preventing fraudulent transactions.

* Built Random Forest, Gradient Boosting with 3-fold cross validation, hyperparameter tunning for 1.8 mil imbalanced data.
* Predicted fraudulent credit card customer transactions by maximizing the F1 score with 90% precision, classification accuracy.

  * **PySpark Libraries used:** pandas, numpy, seaborn, plotly, pyspark.sql, spark.ml
  * **Python libraries used:** pandas, numpy, scipy, sklearn, seaborn, plotly
  * **Input:** 1.8 Million rows consisting Financial and Demographic Information
  * **Output:** Fraudulent and Legitimate Credit Card Transaction Classification with a probability giving optimized F-1 Score


## [AI-Powered Data Insight Platform](https://github.com/AbhijitGokhale/Interactive-LLM-Powered-Data-Analysis-Tool-Full-Stack-AI-App)
**Objective:**  To build an AI-powered web application that enables users to upload datasets and receive instant, context-aware analysis using Large Language Models (LLMs). The app supports session tracking, response history, and dynamic backend processing via a user-friendly frontend.

* Built a full-stack data analysis assistant using JavaScript (UI) and FastAPI (Python backend), enabling users to upload datasets and receive real-time insights via Hugging Face LLM APIs with secure token-based access to Hugging Face models.
* Implemented session tracking to log user activity, supporting personalized history and usage analytics.
* Designed a history view in the UI to persist and display previous LLM responses, improving usability and revisit past insights.

  * **Frontend (JavaScript):** HTML/CSS + Vanilla JavaScript (or optionally React), Axios (for API requests), LocalStorage / IndexedDB (for storing response history)
  * **Backend (Python):** FastAPI – API server and route handling, Pandas – Basic data analysis and manipulation, SQLAlchemy – ORM for SQL database operations, sqlite3 or PostgreSQL – For session/user tracking, Requests – To call Hugging Face APIs, Uvicorn – ASGI server to run FastAPI app
  * **LLM Integration:** Hugging Face Inference API (token-based access to models like gpt2, bert, or bigscience/bloom)


## [Statistical & Predictive Analysis - Vaccination Rates in Californian School Districts](https://github.com/AbhijitGokhale/Statistical-Analysis-on-California-School-Districts-Vaccination)
**Objective:** The project explains specific and appropriate statistical values; both frequentist and Bayesian inferential evidence; explanation including both data exploration and cleaning and appropriate diagnostics.

* Analyzed 15 years incremental trend, seasonality, variations in socioeconomic features through hypothesis driven EDA.
* Determined statistically significant features using hypothesis test, t-test, ANOVA, MCMC Confidence Interval.
* Delivered actionable insights based on linear & logistic regressions with 80% accuracy to improve total vaccination rate 

  * **R Libraries:** tidyverse, ggplot2, dplyr, psych, dlookr, tseries, car, DHARMa, lm.beta, BayesFactor, performance, caret, MCMCpack
  * **Input:** RData file contains two data sets that pertain to vaccinations for the U.S. as a whole and for Californian school districts. The U.S. vaccine data is a time series, and the California data is a sample of end-of-year vaccination reports from n=700 school districts.
  * **Output:** Necessary statistical analysis of vaccination rates in California school districts.


## [USA B2B & B2C Customer Sales Analysis Report 2020 to 2021](https://public.tableau.com/app/profile/abhijit.gokhale/viz/CustomerSalesAnalysisReport2020-2021/CustomerSalesAnalysisReport2020-2021)
**Objective:** Interactive Tableau dashboards with BI reports by leveraging the feasibility to use Built-in and customized visualizations, parameters and fields for a customer transaction data.

* Designed & published 3 interactive Tableau dashboards & BI reports for 280K data using built-in & custom objects.
* Accomplished deliverables to visualize the impact of yearly trends in product categories, order status, payment methods.

  * **Software:** Tableau Desktop
  * **Input:** Excel files - Total 36 columns with 280000 records.
  * **Output:** https://public.tableau.com/app/profile/abhijit.gokhale/viz/CustomerSalesAnalysisReport2020-2021/CustomerSalesAnalysisReport2020-2021


## [Pneumonia Detection Chest XRAY Images using CNN Keras Tensorflow](https://github.com/AbhijitGokhale/Pneumonia-Detection-Chest-XRAY-Images-using-CNN-Keras-Tensorflow)

**Objective:** Chest X-rays are at the moment, the best available method for diagnosing pneumonia, and therefore play a crucial role in diagnosing and providing clinical care to the ones affected. However, detecting pneumonia in chest X-rays is a challenging task that relies on the availability of expert radiologists. Experts are either not available in remote areas or most people can’t afford it. Under such circumstances, automating the detection of diseases through AI becomes the need of the hour. This study will result into aiding healthcare practioners, physicians, doctors, hospitals to take quick actions if the chest Xray detects Pneumonia. (Based on recent studies it's been observed that pneumonia patients are more prone to have COVID symptoms and their ill-effects.) We’ll build an end-to-end machine learning & AI pipeline that uses X-ray images of the lungs to detect pneumonia in patients.

* Preprocessed & normalized 5.8K chest XRAY image data for uniform size, grayscale issue and RGB format.
* Designed CNN using Keras Tensorflow with filter matrix, input filter size patch, MaxPooling, relu, softmax, rmsprop optimzer & binary cross entropy. 
* Delieverd validation & test results to accuratly detect pneumonia from chest XRAY with 2 False Negatives, 74% accuracy & 77% weighted F-1 Score. 

  * **Python Libraries:** open-cv(cv2), keras tensorflow, pandas, numpy, sklearn, glob, matplotlib, pillow(PIL)
  * **Input:** Images in ".jpeg" format for Normal and Pneumonia patients
  * **Output:** An end-to-end machine learning & AI pipeline that uses X-ray images of the lungs to detect pneumonia in patients.
  

## [Covid-19 Patient's Pre-Condition Analysis](https://github.com/AbhijitGokhale/Covid-19-Patient-s-Pre-Condition-Analysis)
**Objective:** Analyze the factors that influence mortality in patients who do not need an intensive care unit and those who do. What are the underlying comorbidities that patients are likely to test positive for COVID? Which factors influence the need for an intensive care unit? This can help with patient triage, optimal distribution of vaccinations (if needed) in countries with limited resources, or prevention in countries susceptible to the virus.

* Analyzed 1 year trend on 560K data and interpreted 3-fold cross validated Random Forest and Gradient Boosting models.
* Showcased actionable insights to treat all age patients by classifying features - ICU requirement, Covid-19 result, and death based on date of symptoms, hospitalization, and patient’s death to reduce the mortality by 70% and save hospitalization costs.

  * **Python Libraries:** pandas, numpy, scipy, sklearn, seaborn, plotly, matplotlib
  * **Input:** Patient's medical pre conditions binomial data
  * **Output:** Classification of requirement of ICU, and classification of chance of getting Covid based on Mortality


## [Tweeter Text Analysis using AWS Neptune](https://github.com/AbhijitGokhale/Tweeter-Text-Analysis-using-AWS-Neptune)
**Objective:** A high performance, scalable, secure, and cost effective NoSQL AWS Neptune database to perform fast information retrieval among edges and nodes.

* Built AWS Neptune graph database by executing Cypher and Gremlin queries in AWS Sagemaker Jupyter Notebook based on 17 nodes and 4 different edges to traverse quickly & recommend the tweets, retweets, and hashtags.
* Facilitated the S3 bucket instance to bulk load data files, and IAM role assignment to provide read-write access. 

  * **Infrastructure:** AWS Cloud
  * **AWS Cloud Services:** S3, IAM, Neptune, VPC, Sagemaker
  * **Input:** Bulk load csv files
  * **Programming Language:** Cypher Query Language, Gremlin
  * **Output:** Graph with 17 nodes and 3 different edges
  * **Project Demo URL:** https://video.syr.edu/media/t/1_n773smyp


## [Soccer Database Management](https://github.com/AbhijitGokhale/Soccer-Database-Management)
**Objective:** Soccer database management projects presents unique MS SQL server backend with frontend developed in Microsoft PowerApps.
* **Data Model:** draw.io for E-R Diagrams and Logical Data Modeling\
* **Relational Databases (Backend Interface):** MS SQL Server with DDL, DML, DCL, and TCL operations
* **Frontend Application UI:** Microsoft PowerApps
* **Backend Application Interface:** MS SQL Server
* **Project Demo URL:** https://video.syr.edu/media/t/1_niyvyvxd


## [Amazon Price Tracker - Web Scraping](https://github.com/AbhijitGokhale/Amazon-Price-Tracker)
**Objective:** Recently there was an Amazon sale and I wanted to buy a product that I had been checking on for a long time. But, as I was ready to buy it I noticed that the price had increased and I wondered if the sale was even legitimate. So I figured by creating this price tracker app, it would not only increase my fluency in python but I would also have my very own home-brewed app to track amazon prices.
* **Python Libraries:** requests,BeautifulSoup, time, smtplib
* **Input:** Product URL, Sender and Receiver Email Adresses (one / many), Price Threshold value 
* **Output:** smtplib to trigger an email with changed price amount
